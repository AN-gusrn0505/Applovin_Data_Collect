from flask import Flask, jsonify
import os
import json
from datetime import datetime, timedelta
import pandas as pd
import requests
from io import StringIO
from google.cloud import bigquery

app = Flask(__name__)

class AxonDataCollector:
    def __init__(self):
        """í™˜ê²½ë³€ìˆ˜ ë¡œë“œ"""
        self.api_key = os.environ['AXON_API_KEY']
        self.project_id = os.environ['GCP_PROJECT_ID']
        self.dataset_id = os.environ['BQ_DATASET_ID']
        self.bq_client = bigquery.Client(project=self.project_id)
        
        # JSON í˜•ì‹ìœ¼ë¡œ ì•± ëª©ë¡ ë¡œë“œ
        apps_config = os.environ.get('APPS_CONFIG', '[]')
        self.apps = json.loads(apps_config)
        print(f"ğŸ“± ë“±ë¡ëœ ì•±: {len(self.apps)}ê°œ")
    
    def check_data_exists(self, table_name, date):
        """íŠ¹ì • ë‚ ì§œ ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        query = f"""
        SELECT COUNT(*) as cnt
        FROM `{self.project_id}.{self.dataset_id}.{table_name}`
        WHERE report_date = '{date}'
        """
        try:
            result = self.bq_client.query(query).result()
            count = list(result)[0].cnt
            return count > 0
        except Exception as e:
            print(f"    âš ï¸ í…Œì´ë¸” í™•ì¸ ì‹¤íŒ¨ ({table_name}): {str(e)}")
            return False
    
    def delete_date_data(self, table_name, date):
        """íŠ¹ì • ë‚ ì§œ ë°ì´í„° ì‚­ì œ (ì—…ë°ì´íŠ¸ ì „)"""
        query = f"""
        DELETE FROM `{self.project_id}.{self.dataset_id}.{table_name}`
        WHERE report_date = '{date}'
        """
        try:
            job = self.bq_client.query(query)
            job.result()
            print(f"    ğŸ—‘ï¸ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ: {date}")
        except Exception as e:
            print(f"    âŒ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
    
    def fetch_user_level_data(self, date, platform, application):
        """User-Level Ad Revenue API í˜¸ì¶œ"""
        print(f"  ğŸ“¥ User-Level ë°ì´í„° ì¡°íšŒ: {application} ({platform})")
        
        url = "https://r.applovin.com/max/userAdRevenueReport"
        params = {
            'api_key': self.api_key,
            'date': date,
            'platform': platform,
            'application': application,
            'aggregated': 'false'
        }
        
        try:
            response = requests.get(url, params=params, timeout=60)
            response.raise_for_status()
            data = response.json()
            
            csv_url = data.get('ad_revenue_report_url') or data.get('url')
            if not csv_url:
                print(f"    âš ï¸ CSV URL ì—†ìŒ")
                return None
            
            csv_response = requests.get(csv_url, timeout=120)
            csv_response.raise_for_status()
            
            df = pd.read_csv(StringIO(csv_response.text))
            if len(df) == 0:
                print(f"    âš ï¸ ë°ì´í„° ì—†ìŒ")
                return None
            
            # ì»¬ëŸ¼ ë§¤í•‘
            column_mapping = {
                'Date': 'impression_timestamp',
                'Ad Unit ID': 'ad_unit_id',
                'Ad Unit Name': 'ad_unit_name',
                'Waterfall': 'waterfall',
                'Ad Format': 'ad_format',
                'Placement': 'placement',
                'Ad Placement': 'ad_placement',
                'Network': 'network',
                'Country': 'country',
                'Device Type': 'device_type',
                'IDFA': 'idfa',
                'IDFV': 'idfv',
                'User ID': 'user_id',
                'Custom Data': 'custom_data',
                'Revenue': 'revenue'
            }
            df.rename(columns=column_mapping, inplace=True)
            
            # í•„ìˆ˜ ì»¬ëŸ¼ ì¶”ê°€
            df['report_date'] = pd.to_datetime(date).date()
            df['application'] = application
            df['package_name'] = application
            df['platform'] = platform
            df['loaded_at'] = datetime.utcnow()
            
            if 'impression_timestamp' in df.columns:
                df['impression_timestamp'] = pd.to_datetime(df['impression_timestamp'])
            if 'revenue' in df.columns:
                df['revenue'] = pd.to_numeric(df['revenue'], errors='coerce')
            
            print(f"    âœ… {len(df)}ê°œ ë…¸ì¶œ ë°ì´í„°")
            return df
            
        except Exception as e:
            print(f"    âŒ ì—ëŸ¬: {str(e)}")
            return None
 
    def fetch_aggregated_revenue(self, date):
        """Revenue Reporting API í˜¸ì¶œ"""
        print(f"  ğŸ“Š Aggregated Revenue ë°ì´í„° ì¡°íšŒ")
        
        url = "https://r.applovin.com/maxReport"
        
        # requests ì œê±° + max_placementë„ ì œê±° (ê°„ì†Œí™”)
        params = {
            'api_key': self.api_key,
            'start': date,
            'end': date,
            'columns': 'day,application,package_name,platform,country,device_type,'
                    'ad_format,impressions,estimated_revenue,ecpm',
            'format': 'csv',
            'not_zero': 1
        }
        
        try:
            response = requests.get(url, params=params, timeout=60)
            response.raise_for_status()
            
            df = pd.read_csv(StringIO(response.text))
            
            # ë””ë²„ê¹…
            print(f"    ğŸ“‹ API ì‘ë‹µ ì»¬ëŸ¼: {df.columns.tolist()}")
            
            if len(df) == 0:
                print(f"    âš ï¸ ë°ì´í„° ì—†ìŒ")
                return None
            
            # ì»¬ëŸ¼ rename
            df.rename(columns={'day': 'report_date'}, inplace=True)
            df['report_hour'] = None
            df['loaded_at'] = datetime.utcnow()
            
            # íƒ€ì… ë³€í™˜
            df['report_date'] = pd.to_datetime(df['report_date']).dt.date
            df['impressions'] = pd.to_numeric(df['impressions'], errors='coerce').fillna(0).astype(int)
            df['estimated_revenue'] = pd.to_numeric(df['estimated_revenue'], errors='coerce').fillna(0).astype(float)
            df['ecpm'] = pd.to_numeric(df['ecpm'], errors='coerce').fillna(0).astype(float)
            
            print(f"    âœ… {len(df)}ê°œ ì§‘ê³„ ë ˆì½”ë“œ")
            return df
            
        except requests.exceptions.HTTPError as e:
            print(f"    âŒ HTTP ì—ëŸ¬: {e}")
            if hasattr(e.response, 'text'):
                print(f"    ğŸ“ ì‘ë‹µ: {e.response.text[:200]}")
            return None
        except Exception as e:
            print(f"    âŒ ì—ëŸ¬: {str(e)}")
            return None


    def load_to_bigquery(self, df, table_name, date, force_update=False):
        """
        DataFrameì„ BigQueryì— ì ì¬
        
        Args:
            df: ì ì¬í•  ë°ì´í„°
            table_name: í…Œì´ë¸”ëª…
            date: ë‚ ì§œ (ì¤‘ë³µ ì²´í¬ìš©)
            force_update: Trueë©´ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ì¬ì ì¬
        """
        if df is None or len(df) == 0:
            return
        
        table_ref = f"{self.project_id}.{self.dataset_id}.{table_name}"
        
        # ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        exists = self.check_data_exists(table_name, date)
        
        if exists and not force_update:
            print(f"    â­ï¸ ì´ë¯¸ ë°ì´í„° ì¡´ì¬, ìŠ¤í‚µ: {date} â†’ {table_name}")
            return
        
        if exists and force_update:
            print(f"    ğŸ”„ ë°ì´í„° ì—…ë°ì´íŠ¸ ëª¨ë“œ: {date}")
            self.delete_date_data(table_name, date)
        
        job_config = bigquery.LoadJobConfig(write_disposition="WRITE_APPEND")
        
        try:
            job = self.bq_client.load_table_from_dataframe(df, table_ref, job_config=job_config)
            job.result()
            print(f"    ğŸ’¾ BigQuery ì ì¬ ì™„ë£Œ: {len(df)}ê°œ â†’ {table_name}")
        except Exception as e:
            print(f"    âŒ BigQuery ì ì¬ ì‹¤íŒ¨: {str(e)}")
    
    def collect_daily_data(self, date=None, apps=None, force_update=False):
        """
        ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸
        
        Args:
            date: ìˆ˜ì§‘ ë‚ ì§œ (ê¸°ë³¸ê°’: ì–´ì œ)
            apps: ì•± ëª©ë¡ (ê¸°ë³¸ê°’: í™˜ê²½ë³€ìˆ˜)
            force_update: ê¸°ì¡´ ë°ì´í„° ìˆì–´ë„ ì—…ë°ì´íŠ¸
        """
        if date is None:
            date = (datetime.utcnow() - timedelta(days=1)).strftime('%Y-%m-%d')
        
        print(f"\n{'='*50}")
        print(f"ğŸ“… {date} ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        if force_update:
            print(f"ğŸ”„ ì—…ë°ì´íŠ¸ ëª¨ë“œ: ê¸°ì¡´ ë°ì´í„° ë®ì–´ì“°ê¸°")
        print(f"{'='*50}")
        
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì•± ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        if apps is None:
            apps = self.apps
        
        if not apps:
            print("âš ï¸ ë“±ë¡ëœ ì•±ì´ ì—†ìŠµë‹ˆë‹¤. APPS_CONFIG í™˜ê²½ë³€ìˆ˜ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            return
        
        # User-Level ë°ì´í„°
        print(f"\n1ï¸âƒ£ User-Level Ad Revenue ìˆ˜ì§‘")
        all_user_data = []
        for app in apps:
            df = self.fetch_user_level_data(
                date=date,
                platform=app['platform'],
                application=app['package']
            )
            if df is not None:
                all_user_data.append(df)
        
        if all_user_data:
            combined_df = pd.concat(all_user_data, ignore_index=True)
            self.load_to_bigquery(combined_df, 'raw_impressions', date, force_update)
        else:
            print("  âš ï¸ User-Level ë°ì´í„° ì—†ìŒ")
        
        # Aggregated Revenue
        print(f"\n2ï¸âƒ£ Aggregated Revenue ìˆ˜ì§‘")
        df_agg = self.fetch_aggregated_revenue(date)
        self.load_to_bigquery(df_agg, 'aggregated_revenue', date, force_update)
        
        print(f"\n{'='*50}")
        print(f"âœ… {date} ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ!")
        print(f"{'='*50}\n")
    
    def backfill_data(self, days=45):
        """
        ê³¼ê±° ë°ì´í„° ì´ˆê¸° ì ì¬ (45ì¼)
        
        Args:
            days: ê³¼ê±° ë©°ì¹ ì¹˜ ë°ì´í„° ìˆ˜ì§‘
        """
        print(f"\n{'ğŸ”¥'*25}")
        print(f"ğŸ”¥ ì´ˆê¸° ë°ì´í„° ì ì¬: ê³¼ê±° {days}ì¼")
        print(f"{'ğŸ”¥'*25}\n")
        
        today = datetime.utcnow()
        
        for i in range(days, 0, -1):
            target_date = (today - timedelta(days=i)).strftime('%Y-%m-%d')
            self.collect_daily_data(date=target_date, force_update=False)
        
        print(f"\n{'ğŸ‰'*25}")
        print(f"ğŸ‰ ì´ˆê¸° ì ì¬ ì™„ë£Œ: {days}ì¼ì¹˜ ë°ì´í„°")
        print(f"{'ğŸ‰'*25}\n")

# Cloud Run ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
@app.route('/', methods=['GET', 'POST'])
def run_collection():
    """Cloud Schedulerì—ì„œ í˜¸ì¶œ (ì¼ì¼ ì—…ë°ì´íŠ¸)"""
    try:
        print("ğŸ“¥ ì¼ì¼ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        collector = AxonDataCollector()
        
        # ì–´ì œ ë°ì´í„°ë§Œ ìˆ˜ì§‘ (ì¤‘ë³µ ì²´í¬ í¬í•¨)
        collector.collect_daily_data(force_update=False)
        
        return jsonify({'status': 'success', 'time': datetime.utcnow().isoformat()}), 200
    except Exception as e:
        print(f"âŒ ì—ëŸ¬: {str(e)}")
        return jsonify({'status': 'error', 'message': str(e)}), 500

@app.route('/backfill', methods=['POST'])
def backfill():
    """ì´ˆê¸° 45ì¼ ë°ì´í„° ì ì¬ (ìˆ˜ë™ í˜¸ì¶œìš©)"""
    try:
        print("ğŸ”¥ ì´ˆê¸° ë°ì´í„° ì ì¬ ì‹œì‘")
        collector = AxonDataCollector()
        collector.backfill_data(days=45)
        return jsonify({'status': 'success', 'message': '45ì¼ ë°ì´í„° ì ì¬ ì™„ë£Œ'}), 200
    except Exception as e:
        print(f"âŒ ì—ëŸ¬: {str(e)}")
        return jsonify({'status': 'error', 'message': str(e)}), 500

@app.route('/force-update', methods=['POST'])
def force_update():
    """ê°•ì œ ì—…ë°ì´íŠ¸ (ì–´ì œ ë°ì´í„° ì¬ìˆ˜ì§‘)"""
    try:
        print("ğŸ”„ ê°•ì œ ì—…ë°ì´íŠ¸ ì‹œì‘")
        collector = AxonDataCollector()
        collector.collect_daily_data(force_update=True)
        return jsonify({'status': 'success', 'message': 'ë°ì´í„° ê°•ì œ ì—…ë°ì´íŠ¸ ì™„ë£Œ'}), 200
    except Exception as e:
        print(f"âŒ ì—ëŸ¬: {str(e)}")
        return jsonify({'status': 'error', 'message': str(e)}), 500

if __name__ == "__main__":
    app.run(host='0.0.0.0', port=int(os.environ.get('PORT', 8080)))